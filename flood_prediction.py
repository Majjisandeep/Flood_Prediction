# -*- coding: utf-8 -*-
"""flood_prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fOXYCoiDttYp5Fe8bueu_CFaiX8iMxhf

# ***WEEK - 1***

# Importing pandas:used for data manipulation, analysis and cleaning
"""

import numpy as np
import pandas as pd

import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    roc_auc_score, confusion_matrix, classification_report, RocCurveDisplay
)
from sklearn.ensemble import RandomForestClassifier

"""# Opening csv file form local drive"""

df = pd.read_csv('flood.csv')
df

"""# nrows Parameter"""

pd.read_csv('flood.csv',nrows=20)

"""# dtypes parameter"""

pd.read_csv('flood.csv',dtype={'target':int}).info()

df.describe()

df.head()

print(df.isnull().sum())

"""# ***WEEK - 2***"""

# Exploratory Data Analysis (EDA)

import matplotlib.pyplot as plt
import seaborn as sns
# Define your target column explicitly
target_col = "FloodProbability"
#1. Target Distribution (Flood / No Flood)
plt.figure(figsize=(15, 5))
sns.countplot(
    x=target_col, hue=target_col, data=df,
    palette="viridis", legend=False
)
plt.title("Distribution of Flood Events")
plt.xlabel(f"{target_col} (0 = No Flood, 1 = Flood)")
plt.ylabel("Count")
print("\n" * 2)
plt.show()
# 2. Quick Overview of Feature Distributions (all numeric features)
df.drop(columns=[target_col]).hist(
    figsize=(12, 10),
    bins=20,
    color="skyblue",
    edgecolor="black"
)
plt.suptitle("Feature Distributions", fontsize=16)
print("\n" * 2)
plt.show()

#3. Correlation Heatmap
plt.figure(figsize=(10, 8))
sns.heatmap(
    df.corr(numeric_only=True),
    annot=True,
    cmap="coolwarm",
    fmt=".2f"
)
plt.title("Feature Correlation Heatmap")
print("\n" * 2)
plt.show()

# Data Transformation
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer

# Load your dataset
df = pd.read_csv('flood.csv')
# Step 1. Drop ID-like columns (if any exist)
drop_cols = [c for c in df.columns if "id" in c.lower()]
if drop_cols:
    df = df.drop(columns=drop_cols)
# Step 2. Handle missing values
imputer = SimpleImputer(strategy="mean")
df[df.columns] = imputer.fit_transform(df)
# Step 3. Encode categorical variables (if present)
# (Skip if all features are numeric; otherwise, encode as needed.)
# Step 4. Define the target and features

target_col = "FloodProbability"
feature_cols = [c for c in df.columns if c != target_col]
# Step 5. Feature scaling
scaler = StandardScaler()
scaled_features = scaler.fit_transform(df[feature_cols])

# Create scaled DataFrame
scaled_df = pd.DataFrame(scaled_features, columns=feature_cols)
scaled_df[target_col] = df[target_col]

# Final check
print("Transformed DataFrame")
print(scaled_df.head())

